{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "#if using CPU training will be slower\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the Size of All Images\n",
    "#All for all images we add the possibiliy of horizontally flipping our images randomly to add VARIATION to our sets\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), #changes to pixel range of each color channel from 0-255 to 0-1 and changes data type from numpy to tensors\n",
    "    #because pyTorch framework takes Tensor as input\n",
    "    transforms.Normalize([.5,.5,.5], #new pixels will be calculate as (x-mean)/standard deviation where x is the old pixel value\n",
    "                         [.5,.5,.5]) #changes range from 0-1 to -1-1\n",
    "    #column represents RGB channel and row represents mean and divisoin, so they are .5 for all channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader helps in reading the data in feeding it into the model for training and matching\n",
    "train_path = \"archive\\seg_train\\seg_train\"\n",
    "test_path = \"archive\\seg_test\\seg_test\"\n",
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path,transform=transformer), #feed in our train directory path and transformer\n",
    "                          batch_size=256,shuffle = True) #batch size should be loaded to the size of my CPU memory\n",
    "test_loader = DataLoader(torchvision.datasets.ImageFolder(test_path,transform=transformer), \n",
    "                          batch_size=256,shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])#fetching all class names and categories and putting them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ConvNet,self).__init__() #specifying all layers in neural network\n",
    "\n",
    "        #output size after convolution filter: (150-3+2(1))/(1) +1\n",
    "\n",
    "        #Input shape= (256,3,150,150) as 256 is the batch size, 3 is the number of channels(RGB), along with height and width\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #shape = (256,12,150,150) since output channels = 12\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12) # adding batch normalization where we are adding the number of features that is equal to number of channels\n",
    "        #shape remains the same\n",
    "        self.reul1=nn.ReLU() #brings non linearity \n",
    "        #shapes remains the same\n",
    "\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2) #reduces height and width of our convolution output by a factor of 2\n",
    "        #shape = (256,12,75,75)\n",
    "\n",
    "        #add another convolution layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #shape = (256,20,75,75) since output channels = 20\n",
    "        self.reul2=nn.ReLU() #brings non linearity \n",
    "        \n",
    "        #add last layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #shape = (256,30,75,75) since output channels = 30\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.reul3=nn.ReLU() #brings non linearity\n",
    "\n",
    "         #add fully connected layer where we feed the number of input features \n",
    "        self.fc=nn.Linear(in_features=32*75*75,out_features=num_classes)\n",
    "\n",
    "        #feed foward function\n",
    "    def forward(self,input): #pass in all inputs from above layers\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.reul1(output)\n",
    "\n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv2(output)\n",
    "        output=self.reul2(output)\n",
    "        \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.reul3(output)\n",
    "\n",
    "        #Output will be in Matrix form with shape of (256,32,75,75)\n",
    "\n",
    "        output=output.view(-1,32*75*75) #in order to feed into full connected layer we reshape into a vector\n",
    "\n",
    "        output=self.fc(output) #feed into fully connected layer \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call Optimizer and loss we use corss entropy\n",
    "optimizer = Adam(model.parameters(),lr=.001,weight_decay=.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14034 3000\n"
     ]
    }
   ],
   "source": [
    "#calculating number of images\n",
    "train_count = len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path+'/**/*.jpg'))\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0Train Loss: tensor(7.6228)Train Accuracy0.5559355850078381Test Accuracy0.5326666666666666\n",
      "Epoch: 1Train Loss: tensor(1.3853)Train Accuracy0.7151204218326921Test Accuracy0.6683333333333333\n",
      "Epoch: 2Train Loss: tensor(1.3030)Train Accuracy0.758016246259085Test Accuracy0.63\n",
      "Epoch: 3Train Loss: tensor(0.6851)Train Accuracy0.8425965512327205Test Accuracy0.7386666666666667\n",
      "Epoch: 4Train Loss: tensor(0.3777)Train Accuracy0.8978195810175289Test Accuracy0.7033333333333334\n",
      "Epoch: 5Train Loss: tensor(0.3008)Train Accuracy0.9142796066695169Test Accuracy0.7256666666666667\n",
      "Epoch: 6Train Loss: tensor(0.1961)Train Accuracy0.9424967935014964Test Accuracy0.7323333333333333\n",
      "Epoch: 7Train Loss: tensor(0.1510)Train Accuracy0.9545389767706998Test Accuracy0.7343333333333333\n",
      "Epoch: 8Train Loss: tensor(0.1063)Train Accuracy0.9719253242126265Test Accuracy0.7\n",
      "Epoch: 9Train Loss: tensor(0.1054)Train Accuracy0.9686475701866895Test Accuracy0.7503333333333333\n"
     ]
    }
   ],
   "source": [
    "#Training our network on our data network loader\n",
    "#Calculate the training loss and accuracy\n",
    "#Save model that gives best accuracy\n",
    "best_accuracy = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Eval and training on training dataset\n",
    "    model.train() #sets in training mode and model keeps some layers that will behave diff\n",
    "    train_accuracy,train_loss = 0.0,0.0\n",
    "    for i, (images,labels) in enumerate(train_loader): #looping inside the batches inside train loaders\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(images.cuda())\n",
    "        optimizer.zero_grad()#back propogation accumulates gradiance we have to zero them out at the begininning of each new batch\n",
    "        outputs = model(images) # gives prediciton\n",
    "        loss = loss_function(outputs,labels) #predicts error\n",
    "        loss.backward()#performs back propogation\n",
    "        optimizer.step() #updates variance and bias\n",
    "\n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "\n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data)) #prediction vs actual value\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count #gives us final accuracy for partricular epoch\n",
    "    train_loss=train_loss/train_count\n",
    "\n",
    "    #Eval on testing dataset\n",
    "    model.eval() #once this is called our model deactives such layer so our model ouput is as expected\n",
    "    test_accuracy = 0.0\n",
    "    for i, (images,labels) in enumerate(test_loader): #looping inside the batches inside test loaders\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(images.cuda())\n",
    "        outputs = model(images) # gives prediciton\n",
    "        _,prediction=torch.max(outputs.data,1) #get the id \n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    test_accuracy=test_accuracy/test_count #gives us test accuracy\n",
    "    print('Epoch: '+str(epoch)+'Train Loss: '+str(train_loss) + 'Train Accuracy ' +str(train_accuracy) + \"Test Accuracy \"+str(test_accuracy))\n",
    "\n",
    "    #Save Best Model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy = test_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
